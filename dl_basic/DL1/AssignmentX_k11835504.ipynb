{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment X- WS 2020 -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular-, Initial- &amp; Normalisation (11 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the bonus assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some tools that can help to speed up the training process of neural networks. **Regularisation** is a technique that can be used to avoid overfitting. Knowing what kind of **initialisation** to use in what context is often important to assure fast learning. **Normalisation** is a tool that tackles the problem of drifting distributions that pops up in very deep networks and hinders learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module, Flatten\n",
    "from nnumpy.testing import gradient_check\n",
    "\n",
    "rng = np.random.default_rng(1856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiser(fn):\n",
    "    \"\"\" \n",
    "    Function decorator for initialisation functions that\n",
    "    enables initialisation of multiple weight arrays at once. \n",
    "    \"\"\"\n",
    "    \n",
    "    def init_wrapper(*parameters, **kwargs):\n",
    "        for par in parameters:\n",
    "            par[:] = fn(par.shape, **kwargs)\n",
    "            par.zero_grad()\n",
    "    \n",
    "    init_wrapper.__name__ = fn.__name__ + \"_init\"\n",
    "    init_wrapper.__doc__ = fn.__doc__\n",
    "    return init_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are infamously prone to overfitting. Just as with any machine learning model, overfitting can relatively easily be detected by monitoring the learning curves on training and validation sets. In order to counter these effects, you can use regularisation techniques. \n",
    "\n",
    "![learning curves](https://d2l.ai/_images/capacity_vs_error.svg)\n",
    "\n",
    "One possibility is to use well-known approaches from regression: e.g. $L_1$ or $L_2$ regularisation, which are also known as *LASSO*, resp. *ridge* regression. Also simply interrupting the learning before the overfitting occurs can prevent overfitting models. These are only a few examples, but most regularisation techniques are not exlusive to neural networks. However, there is one NN-exclusive approach that is very commonly used: **Dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Dropout (3 Points)\n",
    "\n",
    "Dropout is a simple, but very effective regularisation technique that can be added practically anywhere in a network. The idea of dropout is to randomly disable a few neurons during training. During inference all neurons are used. Since this would lead to a shift in distribution of the pre-activations in the next layer (training vs inference), the neurons are scaled down during evaluation so that the distributions during inference and training are approximately the same. In order to avoid the need to change the network during evaluation, it is also possible to scale up the activations during training. This specific change in implementation is often referred to as *inverted* dropout.\n",
    "\n",
    "> Implement the forward and backward pass of an **inverted dropout** module.\n",
    "\n",
    "**Hint:** use the `Module` attribute `predicting` to check whether you are in prediction or training or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Dropout: passed\n"
     ]
    }
   ],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\" NNumpy implementation of (inverted) dropout. \"\"\"\n",
    "\n",
    "    def __init__(self, rate: float = .5, seed: int = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        rate : float, optional\n",
    "            The percentage of neurons to be dropped.\n",
    "        seed : int, optional\n",
    "            Seed for the pseudo random generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rate < 0. or rate > 1.:\n",
    "            raise ValueError(\"dropout rate should be between zero and one\")\n",
    "\n",
    "        self.rate = float(rate)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        # use self.rng, instead of np.random to generate random numbers with the module seed\n",
    "        if self.predicting:\n",
    "            multiplier = np.ones(x.shape)\n",
    "            # raise NotImplementedError(\"TODO: implement prediction mode of Dropout.compute_outputs!\")\n",
    "        else:\n",
    "            multiplier = self.rng.binomial(1, 1-self.rate, x.shape) / (1-self.rate)  # (self.rng.random(x.shape) < self.rate)\n",
    "            #  raise NotImplementedError(\"TODO: implement training mode of Dropout.compute_outputs!\")\n",
    "        return x * multiplier, multiplier\n",
    "\n",
    "    def compute_grads(self, grads, multiplier):\n",
    "        dx = grads * multiplier\n",
    "        return dx\n",
    "        raise NotImplementedError(\"TODO: implement Dropout.compute_grads!\")\n",
    "\n",
    "\n",
    "dropout = Dropout()\n",
    "do_check = gradient_check(dropout, rng.standard_normal(size=(1, 11, 13)), debug=True)\n",
    "print(\"gradient check for Dropout:\", \"passed\" if do_check else \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Dropout: passed\n"
     ]
    }
   ],
   "source": [
    "dropout = Dropout()\n",
    "do_check = gradient_check(dropout, rng.standard_normal(size=(1, 11, 13)), debug=True)\n",
    "print(\"gradient check for Dropout:\", \"passed\" if do_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good initialisation has proven to be very important to learn deep neural networks. Although this can be considered as a well-known fact, it is astonishing how often initialisation is ignored. Since simply initialising all parameters with some constant does not work, the initial values are generally small, randomly generated numbers. There are different distributions to sample these values from, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Xavier Glorot (3 Points)\n",
    "\n",
    "When generating random values, there are different choices for the distribution to draw numbers from. The uniform or Gaussian (a.k.a. normal) distributions are most common for initialising the parameters of a neural network. After all, these are simple distributions that can easily be centred around zero.\n",
    "\n",
    "Apart from centring the initial parameters around zero, it is also helpful to make sure that the weights have a specific amount of variance. Xavier Glorot proposed to use the reciprocal of the average of fan-in and fan-out, i.e. $\\frac{2}{\\text{fan-in} + \\text{fan-out}}$, for the variance. Here, *fan-in* and *fan-out* are the number of incoming connections per output neuron and number of outgoing connections per input neuron, respectively.\n",
    "\n",
    "Note, however, that this proposal only holds for identity and $\\tanh$ activation functions. When using different activation functions, the variance of the initial parameters need to be scaled correspondingly. This can be done by means of a linear *gain* factor that accounts for the effect of the activation functions.\n",
    "\n",
    " > Implement the `glorot_uniform` function so that it produces initial weights for a parameter with given shape according to the proposal from Xavier Glorot. Make sure to make use of the seed for the initialisation, as well as the `gain` parameter.\n",
    " \n",
    "**Hint:** Think carefully about the number of connections in convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@initialiser\n",
    "def glorot_uniform(shape, gain: float = 1., seed: int = None):\n",
    "    \"\"\"\n",
    "    Initialise parameter cf. Glorot, using a uniform distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : tuple\n",
    "        The shape of the parameter to be initialised.\n",
    "    gain : float, optional\n",
    "        Multiplier for the variance of the initialisation.\n",
    "    seed : int, optional\n",
    "        Seed for generating pseudo random numbers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    values: ndarray\n",
    "        Numpy array with the initial weight values\n",
    "        with dimensions as specified by `shape`.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = gain * rng.uniform(size=shape) * (2/(np.sum(shape)))\n",
    "    var = gain * 2/np.sum(shape)\n",
    "    values = var * rng.randn(size=shape)\n",
    "    return values\n",
    "    raise NotImplementedError(\"TODO: implement glorot_uniform function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can include your modules here for the sanity check\n",
    "# e.g. from nnumpy import Linear, Conv2D\n",
    "from nnumpy.nn_modules import Conv2D, Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.random._generator.Generator' object has no attribute 'randn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9f612958864d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mglorot_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1806\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"linear var: {fc.w.var():.2e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  conv var: {conv.w.var():.2e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a6afe763a3ba>\u001b[0m in \u001b[0;36minit_wrapper\u001b[1;34m(*parameters, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-95c39d7029c0>\u001b[0m in \u001b[0;36mglorot_uniform\u001b[1;34m(shape, gain, seed)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TODO: implement glorot_uniform function!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.random._generator.Generator' object has no attribute 'randn'"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "try:\n",
    "    fc = Linear(120, 80)\n",
    "    conv = Conv2D(4, 16, (5, 5))\n",
    "    glorot_uniform(fc.w, conv.w, seed=1806)\n",
    "    print(f\"linear var: {fc.w.var():.2e}\")\n",
    "    print(f\"  conv var: {conv.w.var():.2e}\")\n",
    "except NameError:\n",
    "    print(\"no sanity check...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas initialisation ensures proper variance propagation through the network when learning starts, it does not ensure that the weights keep these properties after some updates. To ensure a steady flow of information through the network, normalisation techniques were introduced. The idea of normalisation is to normalise either the activations or pre-activations. This can be done explicitly, using techniques like *Batch* or *Layer Normalisation*, or more implicitly, e.g. *weight normalisation* or using *self-normalising networks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Batch Normalisation (5 Points)\n",
    "\n",
    "Batch Normalisation (or *batch norm* for short) has empirically proven to be a very useful technique for improving the performance of neural networks. It is not quite clear why it works so well, but there is some form of consensus that it acts as a regulariser and improves gradient flow in the network. \n",
    "\n",
    "The core principle of batch norm is to subtract the mean and divide by the standard deviation of the data, computed over the samples in one batch. Each neuron is normalised individually, so that all neurons have zero mean and unit variance. Batch norm also uses parameters $\\gamma$ and $\\beta$ to scale, resp. shift the normalised signal. Note that, since batch norm relies on batch statistics, it requires a large batch size to work properly!\n",
    "\n",
    "During inference, it is not uncommon to want a prediction for a single sample. Therefore, you generally do not want to use the mean computed during inference. Therefore, batch norm tracks the statistics of the data during training using a [moving average](https://en.wikipedia.org/wiki/Moving_average). During evaluation, these tracked statistics, i.e. the statistics of the training data, are used to normalise the previously unseen samples.\n",
    "\n",
    " > Implement the forward and backward pass of the batch normalisation module. Use a simple moving average for tracking the statistics.\n",
    " \n",
    "**Hint:** You can track the statistics in attributes of the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalisation(Module):\n",
    "    \"\"\" NNumpy implementation of batch normalisation. \"\"\"\n",
    "\n",
    "    def __init__(self, dims: tuple, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dims : tuple of ints\n",
    "            The shape of the incoming signal (without batch dimension).\n",
    "        eps : float, optional\n",
    "            Small value for numerical stability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dims = tuple(dims)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "        self.gamma = self.register_parameter('gamma', np.ones(self.dims))\n",
    "        self.beta = self.register_parameter('beta', np.zeros(self.dims))\n",
    "\n",
    "        self.running_count = 0\n",
    "        self.running_stats = np.zeros((2,) + self.dims)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        if self.predicting:\n",
    "            self.running_stats[0] = np.zeros(x.shape[1:])\n",
    "            self.running_stats[1] = np.ones(x.shape[1:])\n",
    "            # pass\n",
    "            # raise NotImplementedError(\"TODO: implement prediction mode of BatchNormalisation.compute_outputs!\")\n",
    "        else:\n",
    "            x_mean = np.mean(x, axis=0)\n",
    "            x_var = np.var(x, axis=0)\n",
    "            self.running_stats[0] = x_mean\n",
    "            self.running_stats[1] = x_var\n",
    "            # raise NotImplementedError(\"TODO: implement training mode of BatchNormalisation.compute_outputs!\")\n",
    "\n",
    "        x_norm = (x - self.running_stats[0])/np.sqrt(self.running_stats[1] + self.eps)\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        self.running_count = x.shape[0]\n",
    "        cache = x_norm, x\n",
    "        return out, cache\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        x_norm, x = cache\n",
    "        N = self.running_count #  grads.shape[0]\n",
    "\n",
    "        dx_norm = grads * self.gamma  # / np.sqrt(self.running_stats[1] + self.eps)\n",
    "        self.beta.grad = grads.sum(axis=0)\n",
    "\n",
    "        self.gamma.grad = (grads*x_norm).sum(axis=0)\n",
    "\n",
    "        # dx_center = dx_norm / (np.sqrt(self.running_stats[1] + self.eps))\n",
    "        # dmean = -(dx_center.sum(axis=0) + 2/N * (x - self.running_stats[0]).sum(axis=0))\n",
    "        # dstd = (dx_norm * (x - self.running_stats[0]) / (self.running_stats[1] + self.eps)).sum(axis=0)\n",
    "        # dvar = dstd / 2 / (np.sqrt(self.running_stats[1] + self.eps))\n",
    "        # dx = dx_center + (dmean + dvar * 2 * (x - self.running_stats[0])) / N\n",
    "\n",
    "        dx = (1./N) / np.sqrt(self.running_stats[1] + self.eps) * (N * dx_norm - dx_norm.sum(axis=0) - x_norm * (dx_norm * x_norm).sum(axis=0))\n",
    "\n",
    "        return dx\n",
    "        raise NotImplementedError(\"TODO: implement BatchNormalisation.compute_grads!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 4.229421046191073e-18, var: 0.9999999844652488\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "x = rng.uniform(0, 3, size=(7, 3, 5))\n",
    "batch_norm = BatchNormalisation(x.shape[1:])\n",
    "s = batch_norm(x)\n",
    "print(f\"mean: {s.mean()}, var: {s.var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for BatchNormalisation: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "bn_check = gradient_check(batch_norm, x, debug=True)\n",
    "print(\"gradient check for BatchNormalisation:\", \"passed\" if bn_check else \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
