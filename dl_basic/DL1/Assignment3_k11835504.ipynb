{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 3 - WS 2020 -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the third assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with **Convolutional Neural Networks**. Essentially, a CNN is a multi-layer perceptron that uses convolutional instead of fully connected layers. Since convolutions are known to be useful for image processing, CNNs have become a powerful tool for learning features from images. However, they have proven to beat alternative architectures in a variety of other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module\n",
    "from nnumpy.utils import sig2col\n",
    "from nnumpy.testing import gradient_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "The main difference of CNNs with the fully connected networks we tackled thus far, is the *convolution operation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Math\n",
    "\n",
    "Mathematically, a (discrete) convolution operates on two functions, so that\n",
    "\n",
    "$$(f * g)[n] = \\sum_{k \\in \\mathbb{Z}} f[k] g[n - k].$$\n",
    "\n",
    "For image processing, the discrete functions $f$ and $g$ and replaced by images. After all, an image can be considered a function of pixel indices to pixel intensities. Also, images have (at least) two dimensions: width and height. Therefore, if we represent images as matrices of pixel intensities, we can write the convolution of an image $\\boldsymbol{X} \\in \\mathbb{R}^{H \\times W}$ with a so-called *kernel* $\\boldsymbol{K} \\in \\mathbb{R}^{R_1 \\times R_2}$ as follows:\n",
    "\n",
    "$$(\\boldsymbol{K} * \\boldsymbol{X})_{a,b} = \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{i,j} x_{a - i + 1,b - j + 1}.$$\n",
    "\n",
    "Instead of using the actual convolution operation, convolutional layers are often implemented as the *cross-correlation* of kernel and image instead:\n",
    "\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{a,b} = \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{i,j} x_{a + i - 1,b + j - 1}.$$\n",
    "\n",
    "It might be useful to note that unlike the convolution, the cross-correlation is not commutative, i.e. $\\boldsymbol{K} \\star \\boldsymbol{X} \\neq \\boldsymbol{X} \\star \\boldsymbol{K}$, whereas $\\boldsymbol{K} * \\boldsymbol{X} = \\boldsymbol{X} * \\boldsymbol{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Cross-correlation vs Convolution (3 Points)\n",
    "\n",
    "Implementation-wise, there is little difference between cross-correlation and convolution. It is even quite straightforward to implement one, given an implementation of the other. To keep things simple, this exercise is limited to the one-dimensional variants of these operations (for now). How hard would it be to make your implementation of the convolution function commutative?\n",
    "\n",
    "> Implement functions to compute the cross-correlations and convolutions of one-dimensional signals. Obviously, you should **not** use functions like `np.convolve` or `np.correlate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation1d(x, k):\n",
    "    \"\"\"\n",
    "    Compute a one-dimensional cross-correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (L, ) ndarray\n",
    "        Input data for the cross-correlation.\n",
    "    k : (R, ) ndarray\n",
    "        Kernel weights for the cross-correlation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : (L') ndarray\n",
    "        Cross-correlation of the input data with the kernel.\n",
    "    \"\"\"\n",
    "    L = len(x)\n",
    "    R = len(k)\n",
    "    out_len = L - R + 1\n",
    "    \n",
    "    # my solution\n",
    "    feature = np.empty(out_len)\n",
    "    # correlation over vector\n",
    "    for a in range(out_len):\n",
    "        feature[a] = x[a:a + R] @ k\n",
    "    \n",
    "    # solution\n",
    "    feature = [k @ x[a:a+len(k)] for a in range(out_len)]\n",
    "    \n",
    "    return feature\n",
    "    raise NotImplementedError(\"TODO: implement cross_correlation1d function!\")\n",
    "\n",
    "def convolution1d(x, k):\n",
    "    \"\"\"\n",
    "    Compute a one-dimensional convolution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (L, ) ndarray\n",
    "        Input data for the convolution.\n",
    "    k : (R, ) ndarray\n",
    "        Kernel weights for the convolution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : (L', ) ndarray\n",
    "        Result of convolving the input data with the kernel.\n",
    "    \"\"\"\n",
    "    # make reverse vector of kernel\n",
    "    k_flip = np.flip(k) # k[::-1] \n",
    "    feature = cross_correlation1d(x, k_flip)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def cummutative_convolution1d(x, k):\n",
    "    \"\"\"\n",
    "    Compute a one-dimensional convolution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (L, ) ndarray\n",
    "        Input data for the convolution.\n",
    "    k : (R, ) ndarray\n",
    "        Kernel weights for the convolution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : (L', ) ndarray\n",
    "        Result of convolving the input data with the kernel.\n",
    "    \"\"\"\n",
    "    # swap the kernel and input if kernel is larger\n",
    "    if len(k)> len(x):\n",
    "        x, k = k, x\n",
    "\n",
    "    return convolution1d(x, k)\n",
    "    raise NotImplementedError(\"TODO: implement convolution1d function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross correlation check:passed\n",
      "convolution       check:passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "x = np.random.randn(11)\n",
    "k = np.random.randn(3)\n",
    "\n",
    "corr = cross_correlation1d(x, k)\n",
    "corr_check = np.allclose(corr, np.correlate(x, k, mode='valid'))\n",
    "print(\"cross correlation check:\" \"passed\" if corr_check else \"failed\")\n",
    "conv = convolution1d(x, k)\n",
    "conv_check = np.allclose(conv, np.convolve(x, k, mode='valid'))\n",
    "print(\"convolution       check:\" \"passed\" if conv_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Code\n",
    "\n",
    "This direct implementation does not offer a lot of features. For starters, it does not provide functionality to process multiple samples at once. Furthermore, practical implementations of convolutional layers normally require support for *channels*. After all, it is common practice to create multiple feature maps from a single signal to compensate for the spatial reduction through pooling and strides. These features can be incorporated in the mathematical formulation as follows:\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{n,c_\\mathrm{out},a,b} = \\sum_{c_\\mathrm{in}=1}^{C_\\mathrm{in}} \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{c_\\mathrm{out},c_\\mathrm{in},i,j} x_{n,c_\\mathrm{in},a + i - 1,b + j - 1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Of course this makes things a bit more complicated. It also introduces an extra loop over the number of input channels. In order to implement the above formula efficiently, we can use a trick that is commonly referred to as `im2col`. The idea of `im2col` is to represent the input tensor ($\\in \\mathbb{R}^{N \\times C_\\mathrm{in} \\times A \\times B}$) by a tensor in $\\mathbb{R}^{N \\times A' \\times B' \\times (C_\\mathrm{in} \\cdot R_1 \\cdot R_2)}$ where each \"column\" holds the elements in the window of the convolution. This allows the convolution to be computed as a simple matrix product with the (reshaped) kernel matrix $\\boldsymbol{K} \\in \\mathbb{R}^{C_\\mathrm{out} \\times (C_\\mathrm{in} \\cdot R_1 \\cdot R_2)}$, i.e.\n",
    "\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{n,c_\\mathrm{out},a,b} = \\sum_{r=1}^{C_\\mathrm{in} \\cdot R_1 \\cdot R_2} x_{n,a,b,r} k_{r,c_\\mathrm{out}}.$$\n",
    "\n",
    "This trick is (efficiently) implemented in the `sig2col` function (slightly different name, since the function allows for modalities other than images). It takes **two inputs**: the signal to be convolved and the shape of the kernel as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sig2col on 1D signal\n",
    "x = np.arange(7)\n",
    "sig2col(x, (3, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image\n",
    "im = np.arange(16).reshape(4, 4)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  4,  5,  8,  9],\n",
       "        [ 1,  2,  5,  6,  9, 10],\n",
       "        [ 2,  3,  6,  7, 10, 11]],\n",
       "\n",
       "       [[ 4,  5,  8,  9, 12, 13],\n",
       "        [ 5,  6,  9, 10, 13, 14],\n",
       "        [ 6,  7, 10, 11, 14, 15]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3x2 windows in image as column vectors\n",
    "sig2col(im, (3, 2)).reshape(2, 3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-channel Convolutions (3 Points)\n",
    "\n",
    "Time to implement an actually practical convolution function that can handle multiple channels. Let us make it a 2D convolution at once.\n",
    "\n",
    " > Implement the `multi_channel_convolution2d` function below. You can use the `sig2col` function to implement the convolution by means of a dot product.\n",
    " \n",
    "**Hint:** When using the `sig2col` function, you might need to fiddle with the order of dimensions of your numpy arrays to align everything properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_channel_convolution2d(x, k):\n",
    "    \"\"\"\n",
    "    Compute the multi-channel convolution of multiple samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, Ci, A, B)\n",
    "    k : (Co, Ci, R1, R2)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : (N, Co, A', B')\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    sig2col : can be used to convert (N, Ci, A, B) ndarray \n",
    "              to (N, Ci, A', B', R1, R2) ndarray.\n",
    "    \"\"\"\n",
    "    # my solution\n",
    "    N, Ci, A, B = np.shape(x)\n",
    "    Co, Ci, R1, R2 = np.shape(k)  # R1, R2 = k.shape[-2:]\n",
    "    A_prime = A-R1+1\n",
    "    B_prime = B-R2+1\n",
    "    x_view = sig2col(x, (R1, R2))  # sig2col(x, (Ci, R1, R2)) \n",
    "    x_col = x_view.transpose(0,2,3,1,4,5).reshape(N, A_prime, B_prime, -1)\n",
    "    y_col = x_col @ k.transpose(1,2,3,0).reshape(-1, Co)\n",
    "    y = y_col.transpose(0, 3, 1, 2)  # y_col.reshape(N, Co, A_prime, B_prime) #\n",
    "    \n",
    "    # tensordot solution\n",
    "    x_view = sig2col(x, k.shape[-2:])  #\n",
    "    y_col = np.tensordot(x_view, k, axes=([1,4,5],[1,2,3]))\n",
    "    y = y_col.transpose(0,3,1,2)\n",
    "    \n",
    "    # einsum solution\n",
    "    # x_view = sig2col(x, k.shape[-2:])  #\n",
    "    # y = np.einsum('ncabij,ocij->noab', x_view, k)\n",
    "    \n",
    "    return y\n",
    "    raise NotImplementedError(\"TODO: implement multi_channel_convolution2d function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 26, 26)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "x = np.random.randn(10, 1, 28, 28)\n",
    "k = np.random.randn(5, 1, 3, 3)\n",
    "multi_channel_convolution2d(x, k).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Module\n",
    "\n",
    "The multi-channel convolution pretty much covers the forward pass for a typical convolutional layer. For the backward pass, we will need the gradients of this operations. In the case of the simple convolution from the first exercise, it can easily be derived that the gradients w.r.t. inputs and weights are again convolutions, since\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_i} & = \\sum_a \\frac{\\partial L}{\\partial s_a} \\frac{\\partial s_a}{\\partial w_i} = \\sum_a \\delta_a x_{i+a} \\\\\n",
    "    \\frac{\\partial L}{\\partial x_i} & = \\sum_a \\frac{\\partial L}{\\partial s_a} \\frac{\\partial s_a}{\\partial x_i} = \\sum_{a'} w_{a'} \\delta_{i-a'},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial s_a}{\\partial w_i} & = \\frac{\\partial}{\\partial w_i} \\left( \\sum_r w_r x_{a+r} \\right) = x_{a+i} \\\\\n",
    "    \\frac{\\partial s_a}{\\partial x_i} & = \\frac{\\partial}{\\partial x_i} \\left( \\sum_r w_r x_{a+r} \\right) = w_{i - a}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, this approach generalises to multi-channel convolutions. For the convolution of a 1D signal with $c_\\mathrm{i}$ channels so that the output has $c_\\mathrm{o}$ channels, it can be verified that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_{c_\\mathrm{o}, c_\\mathrm{i}, i}} & = \\sum_a \\frac{\\partial L}{\\partial s_{c_\\mathrm{o},a}} \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial w_{c_\\mathrm{o}, c_\\mathrm{i}, i}} = \\sum_a \\delta_{c_\\mathrm{o},a} x_{c_\\mathrm{i},i+a} \\\\\n",
    "    \\frac{\\partial L}{\\partial x_{c_\\mathrm{i}, i}} & = \\sum_{c_\\mathrm{o}} \\sum_a \\frac{\\partial L}{\\partial s_{c_\\mathrm{o},a}} \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial x_{c_\\mathrm{i}, i}} = \\sum_{c_\\mathrm{o}} \\sum_{a'} w_{c_\\mathrm{o}, c_\\mathrm{i}, a'} \\delta_{c_\\mathrm{o}, i-a'},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial w_{c_\\mathrm{o}, c, i}} & = \\frac{\\partial}{\\partial w_{c_\\mathrm{o}, c, i}} \\left( \\sum_{c_\\mathrm{i}} \\sum_r w_{c_\\mathrm{o}, c_\\mathrm{i}, r} x_{c_\\mathrm{i},a+r} \\right) = x_{c, a+i} \\\\\n",
    "    \\frac{\\partial s_{c_1,a}}{\\partial x_{c_2, i}} & = \\frac{\\partial}{\\partial x_{c_2,i}} \\left( \\sum_{c_\\mathrm{i}} \\sum_r w_{c_\\mathrm{o}, c_\\mathrm{i}, r} x_{c_\\mathrm{i}, a+r} \\right) = w_{c_1, c_2, i - a}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can conclude that the gradients of multi-channel convolutions can again be expressed as multi-channel convolutions - taking into account that we compute the convolutions for multiple samples at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Convolutional Layer (5 Points)\n",
    "\n",
    "Now, you should be able to implement both forward and backward pass in a module. Have you already thought about the shape of the bias parameter?\n",
    "\n",
    " > Implement the `Conv2D` module below. You can use the `multi_channel_convolution2d` function from the previous exercise to implement forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Module):\n",
    "    \"\"\" Numpy DL implementation of a 2D convolutional layer. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)        \n",
    "        self.w = self.register_parameter('w', np.empty((out_channels, in_channels, *kernel_size )))  #*kernel_size = kernel_size[0], kernel_size[1]\n",
    "        if use_bias:\n",
    "            self.b = self.register_parameter('b', np.empty(out_channels))  # (out_channels, 1, 1))\n",
    "        # self.b = self.register_parameter('b', np.empty(out_channels if use_bias else 0))\n",
    "        # raise NotImplementedError(\"TODO: register parameters in Conv2D.__init__!\")\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Reset the parameters to some random values. \"\"\"\n",
    "        self.w = np.random.randn(*self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, Ci, H, W) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_maps : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "        feature_map = multi_channel_convolution2d(x, self.w)\n",
    "        if self.use_bias:\n",
    "            feature_map += self.b[None, :, None, None]\n",
    "        cache = x\n",
    "        return feature_map, cache\n",
    "        raise NotImplementedError(\"TODO: implement Conv2D.compute_outputs function!\")\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, Ci, H, W) ndarray\n",
    "        \"\"\"\n",
    "        x = cache\n",
    "        if self.use_bias:\n",
    "            self.b.grad = np.sum(grads, axis=(0, 2, 3))\n",
    "            # solution \n",
    "            # total_grads = np.sum(grads, axis=0)\n",
    "            # self.b.grad = np.sum(total_grads, axis=(-1,2), keepdims=True)\n",
    "            \n",
    "        # my solution\n",
    "        # self.w.grad = multi_channel_convolution2d(x.transpose(1, 0, 2, 3), grads.transpose(1, 0, 2, 3)).transpose(1, 0, 2, 3)\n",
    "        # solution\n",
    "        self.w.grad = multi_channel_convolution2d(x.swapaxes(0,1), grads.swapaxes(0,1)).swapaxes(0,1)\n",
    "        \n",
    "        \n",
    "        # my solution\n",
    "        H_pad = self.kernel_size[0]-1  # x.shape[2] - grads.shape[2]\n",
    "        W_pad = self.kernel_size[1]-1  # x.shape[3] - grads.shape[3]\n",
    "        # grads_pad = np.zeros((grads.shape[0], grads.shape[1], grads.shape[2]+2*H_pad, grads.shape[3]+2*W_pad))\n",
    "        # grads_pad[:, :, H_pad:-H_pad, W_pad:-W_pad] = grads\n",
    "        grads_pad = np.pad(grads, ((0, 0), (0, 0), (H_pad, H_pad), (W_pad, W_pad)))\n",
    "        \n",
    "        # solution\n",
    "        # padding = np.asarray(self.kernel_size) - 1\n",
    "        # grads_pad = np.pad(grads, ((0,0),(0,0)) + tuple(zip(padding, padding)))\n",
    "        \n",
    "        # my solution\n",
    "        # dx = multi_channel_convolution2d(grads_pad, self.w[:, :, ::-1, ::-1].transpose(1, 0, 2, 3)) \n",
    "        # solution \n",
    "        dx = multi_channel_convolution2d(grads_pad, self.w[:, :, ::-1, ::-1].swapaxes(0,1))\n",
    "        return dx\n",
    "        raise NotImplementedError(\"TODO: implement Conv2D.compute_grads function!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Conv2D: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "conv2d = Conv2D(3, 8, (5, 3))\n",
    "conv_check = gradient_check(conv2d, np.random.randn(15, 3, 13, 13), debug=True)\n",
    "print(\"gradient check for Conv2D:\", \"passed\" if conv_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Although any activation function can be used in combination with convolutional neural networks, a very popular choice is the so-called *Rectified Linear Unit* (*ReLU*). The ReLU function maps all negative inputs to zero and all positive inputs to itself. Mathematically, this looks like\n",
    "\n",
    "$$\\mathrm{ReLU}(x) = \\begin{cases} 0 & x \\leq 0 \\\\ x & x > 0 \\end{cases}.$$\n",
    "\n",
    "An alternative activation function that is based on the ReLU, is the *Exponential Linear Unit* (*ELU*). Unlike the ReLU non-linearity, the ELU is able to keep the mean of the activations close to zero. It can be defined as follows:\n",
    "\n",
    "$$\\mathrm{ELU}(x \\mathbin{;} \\alpha) = \\begin{cases} \\alpha (e^x - 1) & x \\leq 0 \\\\ x & x > 0 \\end{cases}.$$\n",
    "\n",
    "The parameter $\\alpha$ in this non-linearity allows to specify the minimal negative value of the activations. Note that this $\\alpha$ is a hyper-parameter that must be fixed before training, and is thus not learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Some Linear Units (2 Points)\n",
    "\n",
    "A deep learning framework would not be complete without the ReLU and ELU activation functions. Time to add them!\n",
    "\n",
    " > Implement the `ReLU` and `ELU` activation function modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\" NNumpy implementation of the Rectified Linear Unit. \"\"\"\n",
    "\n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "\n",
    "        a = np.maximum(0, s)\n",
    "        cache = s\n",
    "        return a, cache\n",
    "        raise NotImplementedError(\"TODO: implement ReLU.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        s = cache\n",
    "        # ds = np.zeros_like(s)\n",
    "        # ds[s > 0] = 1\n",
    "        # ds = grads * ds\n",
    "        ds = np.where(s > 0, 1, 0) * grads\n",
    "        return ds\n",
    "        raise NotImplementedError(\"TODO: implement ReLU.compute_grads method!\")\n",
    "\n",
    "\n",
    "class ELU(Module):\n",
    "    \"\"\" NNumpy implementation of the Exponential Linear Unit. \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__()\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"negative values for alpha are not allowed\")\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        a = np.where(s > 0, s, self.alpha*(np.exp(s)-1))\n",
    "        cache = s\n",
    "        return a, cache\n",
    "        raise NotImplementedError(\"TODO: implement ELU.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        s = cache\n",
    "        ds = np.where(s > 0, 1, self.alpha * np.exp(s)) * grads\n",
    "        return ds\n",
    "        raise NotImplementedError(\"TODO: implement ELU.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for ReLU: passed\n",
      "gradient check for ELU:  passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "relu_check = gradient_check(ReLU(), np.linspace(-3, 3), debug=True)\n",
    "print(\"gradient check for ReLU:\", \"passed\" if relu_check else \"failed\")\n",
    "elu_check = gradient_check(ELU(1.5), np.linspace(-3, 3), debug=True)\n",
    "print(\"gradient check for ELU: \", \"passed\" if elu_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Reduction\n",
    "\n",
    "The *weight sharing* in convolutional neural networks can drastically reduce the memory requirements for the weights. This effectively allows the input data to become larger, but since we need to store parts of the forward pass for back-propagation, the gains are rather limited. Of course, standard convolutions reduce the spatial dimensions, but this linear reduction is often too slow to counter the increased memory requirements due to network depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pooling\n",
    "\n",
    "In order to make working with big images feasible, we need techniques to reduce the spatial dimensions more strongly. This is where *pooling* layers prove very useful. A pooling layer reduces the spatial dimensions by combining a window of pixels to a single pixel. By sticking a pooling layer after every convolutional layer, the spatial dimensions are reduced exponentially, rather than linearly. This allows convolutional neural networks to process big chunks of data.\n",
    "\n",
    "There are different ways to summarise multiple pixels into a single pixel. Two very common pooling techniques are\n",
    "\n",
    " 1. **Average pooling** replaces the pixels by the mean intensity value in the window. \n",
    " 2. **Max pooling** replaces the pixels by the maximum intensity in the window.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Strides\n",
    "\n",
    "In modern convolutional neural networks, *strided* or *dilated* convolutions (see visualisations below) are often preferred over pooling. With strided convolutions, the windows are shifted The main advantage of strided or dilated convolutions over pooling is that they can be learnt. This means that instead of relying on a fixed pooling technique, it is possible to effectively learn how the pixels in the window are to be summarised. Also note that average pooling can indeed be represented as a strided convolution with weights $\\frac{1}{\\text{window size}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <figure style=\"display: inline-block; width: 49%;\">\n",
    "    <img style=\"padding: 46px 50px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\" />\n",
    "    <figcaption style=\"width: 100%;\"> Strided convolution </figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; width: 49%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" />\n",
    "    <figcaption style=\"width: 100%; text-align: center;\"> Dilated convolution </figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "*visualisations taken from the [github](https://github.com/vdumoulin/conv_arithmetic) that comes with [this guide](https://arxiv.org/abs/1603.07285)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Pooling (3 Points)\n",
    "\n",
    "Since the `sig2col` function provides an array with the window elements in each column, it can also be used to implement pooling layers, when used correctly.\n",
    "\n",
    " > Implement the `AvgPool2d` module. You can use the `sig2col` function with its `stride` argument.\n",
    " \n",
    "**Hint:** The gradient of a strided convolution corresponds to a transposed convolution with strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool2d(Module):\n",
    "    \"\"\" Numpy DL implementation of a average pooling layer. \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = tuple(kernel_size)\n",
    "        self.kernel = np.ones((self.kernel_size[0], self.kernel_size[1]))/(self.kernel_size[0]*self.kernel_size[1]) # np.prod(self.kernel_size)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, C, H, W) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        # my solution\n",
    "        # N, C, H, W = x.shape\n",
    "        # R1, R2 = self.kernel_size\n",
    "        # x_col = sig2col(x, self.kernel_size, stride=self.kernel_size).reshape(N, C, H-R1+1, W-R2+1, -1)\n",
    "        # a = x_col @ self.kernel.reshape(-1)\n",
    "        # a = multi_channel_convolution2d(x, self.kernel[None, None, :, :])\n",
    "        # cache = x\n",
    "        \n",
    "        # silly solution\n",
    "        # s = x.reshape(*x.shape[:2], R1, x.shape[-2]//R1, R2, x.shape[-1]//R2)\n",
    "        # a = np.mean(s, axis=(-2, -4))\n",
    "        #  cache = np.array(x.shape)\n",
    "        \n",
    "        # solution \n",
    "        s = sig2col(x, self.kernel_size, stride=self.kernel_size)\n",
    "        a = np.mean(s, axis=(-1, -2))\n",
    "        cache = np.array(x.shape)\n",
    "        \n",
    "        return a, cache\n",
    "        raise NotImplementedError(\"TODO: implement AvgPool2D.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, C, H, W) ndarray\n",
    "        \"\"\"\n",
    "        # my solution (have problem)\n",
    "        # x = cache\n",
    "        # N, C, H_p, W_p = grads.shape\n",
    "        # R1, R2 = self.kernel_size\n",
    "        \n",
    "        # padding to gards\n",
    "        # grads_pad = np.zeros((N, C, H_p+2*(R1-1), W_p+2*(R2-1)))\n",
    "        # grads_pad[:, :, (R1-1):-(R1-1), (R2-1):-(R2-1)] = grads\n",
    "        # grads_pad = np.pad(grads, ((0, 0), (0, 0), (R1-1, R1-1), (R2-1, R2-1)))\n",
    "        \n",
    "\n",
    "        # grads_col = sig2col(grads_pad, self.kernel_size).reshape(N, C, H_p+R1-1, W_p+R2-1, -1)\n",
    "        # dx = grads_col @ self.kernel.reshape(-1)\n",
    "        ## dx = multi_channel_convolution2d(grads_pad, self.kernel[None, None, :, :])\n",
    "        \n",
    "        # solution\n",
    "        padding = np.asarray(self.kernel_size) - 1\n",
    "        padded_shape = tuple(cache[:-2]) + tuple(cache[-2:] + padding)\n",
    "        _grads = np.zeros(padded_shape, dtype=grads.dtype)\n",
    "        idx = (..., ) + tuple(slice(p, -p, k) for p, k in zip(padding, self.kernel_size))\n",
    "        _grads[idx] = grads / np.prod(self.kernel_size)\n",
    "        grads_col = sig2col(_grads, self.kernel_size)\n",
    "        dx = np.sum(grads_col, axis=(-1, -2))\n",
    "\n",
    "        return dx\n",
    "        raise NotImplementedError(\"TODO: implement AvgPool2D.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for AvgPool2D: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "pooling = AvgPool2d((2, 3))\n",
    "pool_check = gradient_check(pooling, np.random.randn(1, 1, 16, 18), debug=True)\n",
    "print(\"gradient check for AvgPool2D:\", \"passed\" if pool_check else \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
