{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 2 - WS 2020 -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons (15 points + 2 bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the second assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with **Multi-Layer Perceptrons**. Essentially, MLPs are the result of stacking one or more of the simple networks from last time, interleaved with some form of non-linearities. This hints at some form of modular implementation, which will lead to the first building blocks of your very own deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module, Container, LossFunction\n",
    "from nnumpy.testing import gradient_check\n",
    "from nnumpy.utils import to_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module System\n",
    "\n",
    "Multi-layer perceptrons can essentially be assembled from simple networks and activation functions. This kind of modularity is a recurring theme in deep learning, so much so that most deep learning frameworks are implemented in a modular fashion. This allows to construct complex networks from relatively simple building blocks. On top of that, it breaks down the possibly complex backprop derivation in more digestible pieces.\n",
    "\n",
    "In order to kick-start your deep learning library, we provide you with the `Module` class, which implements some plumbing and python magic in an attempt to make them easier to use. In order for your models to inherit this functionality, they should be subclasses of `Module` and implement the functions `compute_outputs` and `compute_grads`. The first function should compute the result of the forward pass and collect the values that will be necessary to compute the gradients in the backward pass. The backward pass is to be implemented in the second function, which comes down to computing all possible gradients. The following schematic illustrates this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, `compute_outputs` takes one or more inputs and should return two values: the output of the module from the input and the `cache`, which contains the values necessary for the backward pass. `compute_grads` expects the gradients from the module that uses the outputs from this module as inputs, as well as the `cache` from the forward pass as inputs. This function should compute the gradients w.r.t. the module's parameters, but also the gradients w.r.t. inputs, which are to be returned by `compute_grads`. See the example module `Example` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(Module):\n",
    "    \"\"\" \n",
    "    Example of what a NNumpy module could look like.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    This module can be used as follows:\n",
    "    >>> module = Example()\n",
    "    >>> x = np.random.randn(10)\n",
    "    >>> pred = module.forward(x, x)\n",
    "    >>> module.zero_grad()  # initialise gradients\n",
    "    >>> dx, dx = module.backward(np.ones_like(pred))\n",
    "    >>> print(module.scale.grad)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # very important for automagic, always call super init!\n",
    "        super().__init__()\n",
    "        \n",
    "        # add scalar parameter for fun\n",
    "        self.scale = self.register_parameter('scale', 2.)\n",
    "        \n",
    "    def compute_outputs(self, s, t):\n",
    "        \"\"\"\n",
    "        Compute outputs for this module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input0, input1, ..., inputn : ndarray\n",
    "            One or more inputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : ndarray or tuple of ndarrays\n",
    "            One or more module outputs.\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "            One or more values that are necessary for the gradient computation.\n",
    "        \"\"\"\n",
    "        # compute output value\n",
    "        a = self.scale * s * t\n",
    "        \n",
    "        # collect info for backward pass\n",
    "        cache = s, t\n",
    "        \n",
    "        return a, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Compute gradients for this module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : ndarray\n",
    "            Gradients from subsequent module.\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "            Cached values from the forward pass.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grad0, grad1, ..., gradn : ndarray\n",
    "            Gradients w.r.t. to the input(s).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Updates the parameter gradients.\n",
    "        \"\"\"\n",
    "        # collect info from forward pass\n",
    "        s, t = cache\n",
    "        \n",
    "        # compute parameter gradient\n",
    "        self.scale.grad = np.sum(s * t)\n",
    "        \n",
    "        # compute gradients w.r.t. inputs\n",
    "        ds = grads * t\n",
    "        dt = grads * s\n",
    "        \n",
    "        return ds, dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Activation Function Module (2 Points)\n",
    "\n",
    "Without non-linearities, Multi-Layer Perceptrons would make little sense. After all, multiple linear layers can always be reduced to a single-layer linear network, i.e. linear regression. Essentially any non-linear function could serve as activation function, but in practice only a few functions with specific properties are considered. \n",
    "\n",
    "Since most activation functions do not have any parameters, their modules are pretty straightforward to implement. Consider this first exercise as an opportunity to get familiar with the module system.\n",
    "\n",
    "> Implement the forward and backward pass for the `Identity` and `Tanh` activation function modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Module):\n",
    "    \"\"\" NNumpy implementation of the identity function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # compute output value\n",
    "        a = s\n",
    "        # collect info for backward pass\n",
    "        cache = s\n",
    "        return a, cache\n",
    "        raise NotImplementedError(\"TODO: implement Identity.compute_outputs method!\")\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        s = cache\n",
    "        ds =  grads * np.ones_like(s)\n",
    "        return ds\n",
    "        raise NotImplementedError(\"TODO: implement Identity.compute_grads method!\")\n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "    \"\"\" NNumpy implementation of the hyperbolic tangent function. \"\"\"\n",
    "    \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        a = np.tanh(s)\n",
    "        cache = s, a\n",
    "        return a, cache\n",
    "        raise NotImplementedError(\"TODO: implement Tanh.compute_outputs method!\")\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        s, a = cache\n",
    "        ds = grads * (1-a**2)  # (1-np.tanh(s)**2)\n",
    "        return ds\n",
    "        raise NotImplementedError(\"TODO: implement Tanh.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "It is not uncommon to make mistakes when computing gradients - or implementing them - in neural networks. In order to catch possible issues with the backward pass in deep learning, a technique called *gradient checking* is often used. To check the implementation of an analytically derived gradient, a numerical approximation is used to check the analytic gradient implementation against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different methods to approximate gradients numerically, but one of the easiest is probably to use finite difference approximations. This method directly uses the definition of a derivative, \n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x + h) - f(x)}{h},$$\n",
    "\n",
    "which can also be written as\n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x) - f(x - h)}{h}$$\n",
    "\n",
    "or, for the sake of symmetry, as the average of the two expressions above:\n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}.$$\n",
    "\n",
    "In the end, a finite difference approximation is nothing more than one of the above formulas without limits, i.e. for some value of $h$. In practice, $h$ is set to some small value, often referred to as $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Checking (3 Points)\n",
    "  \n",
    "Although gradient checking normally does not appear in the interface of deep learning frameworks, it is an important tool to make sure implementations are correct. Therefore, we will start off by implementing a very simple gradient checker.\n",
    "\n",
    "> Implement the symmetric gradient approximation and a simple gradient checker that is able to check the gradients of simple modules like the activation functions from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff(func, x, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the symmetric difference quotient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function to compute the difference quotient for.\n",
    "    x : ndarray\n",
    "        The points in which the gradient should be computed.\n",
    "    eps : float, optional\n",
    "        The epsilon perturbation used in the difference computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    grad : ndarray\n",
    "        The perturbation\n",
    "    \"\"\"\n",
    "    grad = (func(x+eps)[0] - func(x-eps)[0]) / (2*eps)\n",
    "    return grad\n",
    "    raise NotImplementedError(\"TODO: implement finite_diff function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gradient_check(module, x, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Check if the input gradients of an activation function module are correct.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    module : Module\n",
    "        The activation function module to check.\n",
    "    x : ndarray\n",
    "        The inputs for which the gradient should be computed.\n",
    "    eps : float, optional\n",
    "        The small perturbation used in the finite difference computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    success : bool\n",
    "        Whether or not the gradient check passed.\n",
    "    \"\"\"\n",
    "    grads = np.ones_like(x)\n",
    "    dx_analytic = module.compute_grads(grads, x)\n",
    "    dx_numeric = np.empty_like(x)\n",
    "    for ii, xi in enumerate(x.flat):\n",
    "        dx_numeric[ii]=finite_diff(module.compute_outputs, xi, eps)\n",
    "                   \n",
    "    error = np.mean((dx_analytic - dx_numeric)**2)\n",
    "    if error<eps:\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "            \n",
    "    return success\n",
    "    # Hint: use module.forward and module.backward to get the analytical gradients\n",
    "    raise NotImplementedError(\"TODO: implement my_gradient_check function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Square:   passed\n",
      "gradient check for Identity: passed\n",
      "gradient check for Tanh:     passed\n"
     ]
    }
   ],
   "source": [
    "class Square(Module):\n",
    "    \"\"\" NNumpy module to show effectiveness of gradient checking. \"\"\"\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        return x * x, x\n",
    "    \n",
    "    def compute_grads(self, grads, x):\n",
    "        return grads * 2 * x\n",
    "\n",
    "square_check = my_gradient_check(Square(), np.linspace(-5, 5))\n",
    "print(\"gradient check for Square:  \", \"passed\" if square_check else \"failed\")\n",
    "id_check = my_gradient_check(Identity(), np.linspace(-3, 3))\n",
    "print(\"gradient check for Identity:\", \"passed\" if id_check else \"failed\")\n",
    "tanh_check = my_gradient_check(Tanh(), np.linspace(-3, 3))\n",
    "print(\"gradient check for Tanh:    \", \"passed\" if tanh_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "\n",
    "One of the key components in a multi-layer perceptron is the fully connected layer. It can be implemented in a fairly simple module with a weight matrix and bias vector as parameters. The `Module` base-class provides a method `Module.register_parameter(name, value)` that auto-magically creates an attribute `Module.<name>` for the module. This attribute has the type `Parameter`, which is essentiall a numpy array with an additional attribute `Parameter.grad` to store its gradients. In order to make sure that the parameters are correctly initialised, use the method `Module.reset_parameters()`. For initialising the gradients, there is also a method, `Module.zero_grad()`, that should be called after every update. Take the following code as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41666829e-311 0.00000000e+000 2.12131884e+289] [0. 0. 0.]\n",
      "None [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "m = Module()\n",
    "\n",
    "# create attributes\n",
    "m.register_parameter('theta', np.empty(3))\n",
    "\n",
    "# parameter attribute (Note that parameters are uninitialised!)\n",
    "print(m.theta, end=' ')\n",
    "m.reset_parameters()  # initialise parameters\n",
    "print(m.theta)\n",
    "\n",
    "# parameter gradient attribute\n",
    "print(m.theta.grad, end=' ')\n",
    "m.zero_grad()\n",
    "print(m.theta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: A Module with Parameters (4 Points)\n",
    "\n",
    "Remember the linear regression from the first assignment? It's time to pour that code into a module and check whether you can correctly propagate gradients! \n",
    "\n",
    "> Implement the `Linear` module, which represents a fully connected layer. The derivatives w.r.t. the parameters can be stored in the `grad` attribute of registered parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    NNumpy implementation of a fully connected layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features (D) this layer expects.\n",
    "    out_features : int\n",
    "        Number of output features (K) this layer expects.\n",
    "    use_bias : bool\n",
    "        Flag to indicate whether the bias parameters are used.\n",
    "\n",
    "    w : Parameter\n",
    "        Weight matrix.\n",
    "    b : Parameter\n",
    "        Bias vector.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fc = Linear(10, 1)\n",
    "    >>> fc.reset_parameters()  # init parameters\n",
    "    >>> s = fc.forward(np.random.randn(1, 10))\n",
    "    >>> fc.zero_grad()  # init parameter gradients\n",
    "    >>> ds = fc.backward(np.ones_like(s))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)\n",
    "        self.w = self.register_parameter('w', np.empty((out_features, in_features))) # np.zeros((self.out_features, self.in_features))\n",
    "        self.b = self.register_parameter('b', np.empty(out_features if use_bias else 0))\n",
    "#         if self.use_bias:\n",
    "#             self.b = np.zeros(self.out_features)\n",
    "        # raise NotImplementedError(\"TODO: register parameters in Linear.__init__!\")\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Set initial values for parameters. \"\"\"\n",
    "        self.w = np.random.randn(*self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.random.randn(*self.b.shape)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : (N, K) ndarrays\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        s = x @ self.w.T\n",
    "        if self.use_bias:\n",
    "            s += self.b\n",
    "        cache = x\n",
    "        return s, cache\n",
    "        raise NotImplementedError(\"TODO: implement Linear.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarrays\n",
    "        \"\"\"\n",
    "        x = cache\n",
    "        if self.use_bias:\n",
    "            self.b.grad = np.sum(grads, axis=0)\n",
    "        \n",
    "        self.w.grad = grads.T @ x\n",
    "        dx = grads @ self.w\n",
    "        return dx\n",
    "        raise NotImplementedError(\"TODO: implement Linear.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Linear: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "linear = Linear(19, 7)\n",
    "linear_check = gradient_check(linear, np.random.randn(100, 19), debug=True)\n",
    "print(\"gradient check for Linear:\", \"passed\" if linear_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "A multi-layer perceptron is essentially a stack of single-layer networks with some sort of non-linearity. With the `Linear` module and activation function modules, we have all ingredients to construct MLPs and make learning *deep*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Chaining modules (3 Points)\n",
    "\n",
    "In essence, MLPs can be constructed by chaining the modules in the right order. Since this is a common pattern in deep learning architectures, it makes sense to make a general module for chaining other modules. \n",
    "\n",
    "> Implement forward and backward pass for the `Sequential` module so that it comes down to chaining all its sub-modules.\n",
    "\n",
    "**Hint:** the cache will probably not be a single numpy array, here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Container):\n",
    "    \"\"\"\n",
    "    NNumpy module that chains together multiple one-to-one sub-modules.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Doubling a module could be done as follows:\n",
    "    >>> module = Module()\n",
    "    >>> seq = Sequential(module, module)\n",
    "    \n",
    "    Modules can be accessed by index or by iteration:\n",
    "    >>> assert module is seq[0] and module is seq[1]\n",
    "    >>> mod1, mod2 = (m for m in seq)\n",
    "    >>> assert mod1 is module and mod2 is module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        if len(modules) == 1 and hasattr(modules[0], '__iter__'):\n",
    "            modules = modules[0]\n",
    "        \n",
    "        for i, mod in enumerate(modules):\n",
    "            self.add_module(mod)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N, K) ndarrays\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # simple solution of teacher\n",
    "#         for module in self:\n",
    "#             y = module.forward(x)\n",
    "#         return y, None\n",
    "######################################\n",
    "        cache = list()\n",
    "#         s = x\n",
    "#         for cnt, mod in enumerate(self._modules):\n",
    "#             s, c = self._modules[cnt].compute_outputs(s)\n",
    "#             cache.append(c)\n",
    "#         y = s\n",
    "        for module in self:\n",
    "            x,c = module.compute_outputs(x)\n",
    "            cache.insert(0,c)\n",
    "        y = x\n",
    "        return y, cache\n",
    "\n",
    "        raise NotImplementedError(\"TODO: implement Sequential.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarrays\n",
    "        \"\"\"\n",
    "        # simple solution of teacher\n",
    "#        for module in reversed(self):\n",
    "#             grads = module.backward(grads)\n",
    "###############################################\n",
    "#         for cnt, mod in reversed(list(enumerate(self._modules))):\n",
    "#             grads = self._modules[cnt].compute_grads(grads, cache[cnt])\n",
    "#         dx = grads\n",
    "#         return dx\n",
    "        \n",
    "        for module, c in zip(reversed(self), cache):\n",
    "            grads = module.compute_grads(grads, c)\n",
    "        return grads\n",
    "        # Hint: you can access the list of modules through `self._modules`\n",
    "        raise NotImplementedError(\"TODO: implement Sequential.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "class MLP(Sequential):\n",
    "    \"\"\" NNumpy implementation of Multi-Layer Perceptron. \"\"\"\n",
    "    \n",
    "    def __init__(self, *features, act_func=None, use_bias=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        f1, f2, ..., fn : int\n",
    "            Number of neurons in each layer.\n",
    "            f1 is the number of neurons in the input-layer.\n",
    "            fn is the number of neurons in the output-layer.\n",
    "        act_func : Module, optional\n",
    "            Module to use as activation function.\n",
    "            If not specified, the model is linear.\n",
    "        use_bias : bool, optional\n",
    "            Whether or not each layer should have a bias term.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = features[0]\n",
    "        self.out_features = features[-1]\n",
    "        self.use_bias = use_bias\n",
    "        self.phi = act_func or Identity()\n",
    "        \n",
    "        for n_in, n_out in zip(features[:-2], features[1:-1]):\n",
    "            self.add_module(Linear(n_in, n_out, use_bias))\n",
    "            self.add_module(self.phi)\n",
    "        \n",
    "        self.output_layer = Linear(features[-2], features[-1], use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for MLP: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "mlp = MLP(19, 7, 11, 3, act_func=Tanh())\n",
    "mlp_check = gradient_check(mlp, np.random.randn(100, 19), debug=True)\n",
    "print(\"gradient check for MLP:\", \"passed\" if mlp_check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Necessity for Non-linearity (1 Point)\n",
    "\n",
    "As already mentioned, a multi-layer perceptron without non-linearities is equivalent to a single-layer network. How is that?\n",
    "\n",
    "> Set the weights for a single-layer perceptron so that it produces the same results as the given multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are non-linearities necessary? yes\n"
     ]
    }
   ],
   "source": [
    "# multi-layer perceptron\n",
    "mlp = Sequential(\n",
    "    Linear(23, 17),\n",
    "    Linear(17, 19),\n",
    "    Linear(19, 7)\n",
    ")\n",
    "\n",
    "# single-layer perceptron\n",
    "slp = Linear(23, 7)\n",
    "slp.w = mlp._modules[2].w @ mlp._modules[1].w @ mlp._modules[0].w  # TODO: set weight value here\n",
    "slp.b = mlp._modules[2].b + mlp._modules[2].w @ (mlp._modules[1].b + mlp._modules[1].w @ mlp._modules[0].b)  # TODO: set bias value here\n",
    "\n",
    "# test equivalence\n",
    "x = np.linspace(-3, 3, 23)\n",
    "equivalence_check = np.allclose(slp.forward(x), mlp.forward(x))\n",
    "print(\"Are non-linearities necessary?\", \"yes\" if equivalence_check else \"not for me...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Modules\n",
    "\n",
    "Also loss functions fit in the module system. Loss functions take two inputs and can normally be returned in one of three ways:\n",
    "\n",
    "  1. An array of individual sample errors\n",
    "  2. The total sample error(s)\n",
    "  3. The average sample error(s)\n",
    "  \n",
    "To facilitate these three options, a `LossFunction` module has been provided. This module accepts a keyword argument `reduction` that allows to specify how to *reduce* the errors for different sampels (options are respectively `'none'`, `'sum'` or `'mean'`). For gradient checking, you want to use `'none'`, but in practice it is more common to use something like `'mean'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Logit Cross-Entropy (2 Points + 2 Bonus Points)\n",
    "\n",
    "The last assignment required you to implement the `softmax` and `cross_entropy` functions separately. In most deep learning frameworks, these functions are merged into a single loss function. The main reason is numerical stability, but it also simplifies the computation of the gradients.\n",
    "\n",
    "> Implement a module, i.e. forward and backward pass, that computes the cross-entropy from pre-activations and one-hot targets. E.g. by first computing softmax and then applying cross-entropy. Don't forget to compute the gradients w.r.t. targets! \n",
    "\n",
    "For the bonus point, you would have to\n",
    "\n",
    "> implement the numerically stable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitCrossEntropy(LossFunction):\n",
    "    \"\"\"\n",
    "    NNumpy implementation of the cross entropy loss function\n",
    "    computed from the logits, i.e. before applying the softmax nonlinearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def _softmax_grad_func(self, x, y, w, b):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the cross-entropy w.r.t. the parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "            Inputs to the network.\n",
    "        y : (N, K) ndarray\n",
    "            Targets for the network.\n",
    "        w : (K, D) ndarray\n",
    "            Input weights of the network.\n",
    "        b : (K, ) ndarray\n",
    "            Bias weights of the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        err : (N, ) ndarray\n",
    "            The error of the prediction from the network \n",
    "            for the given input data before the update.\n",
    "        grads : ((K, D) ndarray, (K, ) ndarray)\n",
    "            Gradients of the logistic error function w.r.t. the parameters,\n",
    "            i.e. `dw` and `db` (in that order).\n",
    "        \"\"\"\n",
    "        logits = my_first_network(x, w, b)\n",
    "        pred = softmax(logits)\n",
    "        err = cross_entropy(pred, y)\n",
    "\n",
    "        ds = pred - y\n",
    "        dw = ds.T@x\n",
    "        db = np.sum(ds, axis=0)\n",
    "        return err, (dw, db)\n",
    "\n",
    "    def _softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Compute the softmax function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "            The logits to apply the softmax function on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "            The probabilitie(s) for the given logit(s).\n",
    "        \"\"\"\n",
    "        # Hint: check the row sums!      \n",
    "        exp_logits = np.exp(logits-np.max(logits))\n",
    "        a = exp_logits / exp_logits.sum(axis=-1, keepdims=True)\n",
    "        return a\n",
    "        raise NotImplementedError(\"TODO: implement softmax function!\")\n",
    "\n",
    "    def _cross_entropy(self, pred, truth):\n",
    "        \"\"\"\n",
    "        Compute the logistic error of a predicted value, given the actual target.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction : (N, K) ndarrary\n",
    "            The value(s) predicted by the model.\n",
    "        truth : (N, K) ndarray\n",
    "            The actual target(s) from the data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error : (N, ) ndarray\n",
    "            The logistic error(s) for the prediction(s).\n",
    "        \"\"\"\n",
    "        # Hint: check the sign!\n",
    "        error = - np.sum(truth * np.log(pred), axis=-1)\n",
    "        return error\n",
    "        raise NotImplementedError(\"TODO: implement cross_entropy function!\")\n",
    "\n",
    "    def compute_outputs(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : (N, K) ndarray\n",
    "        targets : (N, K) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cross_entropy : (N, 1) or (1, ) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        preds = self._softmax(logits)\n",
    "        cross_entropy = self._cross_entropy(preds, targets)\n",
    "        cache = (preds, targets)\n",
    "        return cross_entropy, cache\n",
    "        raise NotImplementedError(\"TODO: implement LogitCrossEntropy.compute_outputs method!\")\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, 1) or (1, ) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dlogits : (N, K) ndarray\n",
    "        dtargets : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        preds, targets = cache\n",
    "        dlogits = grads[:, None] * (preds - targets)\n",
    "        dtarget = grads[:, None] * -np.log(preds)  \n",
    "        return dlogits, dtarget\n",
    "        raise NotImplementedError(\"TODO: implement LogitCrossEntropy.compute_grads method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for LogitCrossEntropy: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "lce = LogitCrossEntropy(reduction='none')\n",
    "targets = to_one_hot(np.random.randint(7, size=100))\n",
    "lce_check = gradient_check(lce, np.random.randn(100, 7), targets, debug=True)\n",
    "print(\"gradient check for LogitCrossEntropy:\", \"passed\" if lce_check else \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
