{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 3: Expectation Maximization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Proof details for EM-algorithm</h2>\n",
    "\n",
    "This task is meant to foster your understanding concerning some theoretical insights into the EM algorithm.\n",
    "As you have seen in the lecture, when doing EM, you improve the log likelihood of the data $\\mathsf{ln} p(\\mathbf{x}\\!\\mid\\!\\mathbf{\\Theta})$ by introducing a distribution $Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})$ on hidden variables $\\mathbf{u}$. Then, instead of maximizing the log likelihood directly, you maximize a lower bound which is obtained through Jensen's inequality. We have:\n",
    "\\begin{align}\n",
    " \\mathsf{ln} (p(\\mathbf{x})) \\geq  \\int_U Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})\\ \\mathsf{ln} \\frac{p(\\mathbf{x}, \\mathbf{u})}{Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})}\\ d\\mathbf{u} \\ . \\quad (1)\n",
    "\\end{align}\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Task:</h3>\n",
    "\n",
    "Confirm that after each E-step in the EM algorithm, this lower bound is reached with equality (\"the bound is tight\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation (20 points):</h3>\n",
    "\n",
    "Your calculation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Comment:</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous version it was wrongly stated that (1) can be written equivalently as:\n",
    "\\begin{align}\n",
    "\\sum_i^N \\mathsf{ln} (p(\\mathbf{x}^i)) \\geq \\sum_i^N \\int_U Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})\\ \\mathsf{ln} \\frac{p(\\mathbf{x}^i, \\mathbf{u})}{Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})}\\ d\\mathbf{u} \\ \n",
    "\\end{align}\n",
    "when $\\mathbf{x}^i$ are the individual data points, which are assumed to be i.i.d. However, using the i.i.d. assumptions, the inequality (1), which follows from the results from the lecture, is only equivalent to: \n",
    "\\begin{align}\n",
    "\\sum_i^N \\mathsf{ln} (p(\\mathbf{x}^i)) \\geq  \\int_U  Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})\\ \\left\\{ \\left( \\sum_i^N \\mathsf{ln} (p(\\mathbf{x}^i, \\mathbf{u}) \\right)+\\mathsf{ln} \\frac{1}{Q(\\mathbf{u}\\!\\mid\\!\\mathbf{x})} \\right\\} d\\mathbf{u}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\"> Task 2: Examples of Mixture models </h2>\n",
    "\n",
    "In the slides, we discussed the procedure of obtaining well-suited parameters for a mixture of Gaussians model, thereby using the EM-algorithm. Now we want to apply these techniques to derive update formulas for a similar (and even easier) tasks in the discrete setting, namely the Mixture of (one-dimensional) Poisson distributios and Bernoulli distributions. Let us start recalling the general notion of a mixture model: a mixture of $K$ distributions $p_k(x;\\theta_k)$ with $k=1,...,K$ can be written as:\n",
    "$\n",
    "p(x) = \\sum_{k=1}^K\\alpha_k p_k(x;\\theta_k),\n",
    "$\n",
    "where $\\sum_{k=1}^K\\alpha_k = 1$ and all $\\alpha_k > 0$. If the data are assumed to be drawn i.i.d. from $p(x)$, the likelihood function has the form\n",
    "\\begin{equation}\n",
    "\\mathsf{ln} \\mathcal{L}(\\Theta \\!\\mid\\! x_1, ..., x_n) = \\mathsf{ln} \\prod_{i=1}^n p(x_i) = \\sum_{i=1}^n \\mathsf{ln}\\Big(\\sum_{k=1}^K \\alpha_k p(x_i;\\theta_k)\\Big)\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{ \\alpha_1, ..., \\alpha_K, \\theta_1, ..., \\theta_K \\}$.\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Task:</h3>\n",
    "\n",
    "In a similar way as in the slides apply the EM-algorithm, specifically you are asked to derive update formulas for $r_{ik}$, $\\alpha_k$, $\\lambda_k$ and $\\mu_k$ (the corresponding parameters) to be used in the E- and M-steps for:\n",
    "* A mixture of Poisson distributions $P(x;\\lambda) = \\lambda^{x}e^{-\\lambda}\\frac{1}{x!}$. Keep in mind that this is a discrete distribution, i.e. $x \\in \\mathbb{N} \\cup \\{ 0 \\}$. Moreover, $\\lambda>0$.\n",
    "* A mixture of Bernoulli distributions $P(x; \\mu)= \\mu ^x (1-\\mu)^{1-x}$. Keep in mind that this is a discrete distribution with binary outputs, such that in our case $x \\in  \\{ 0,1 \\}$. Moreover $\\mu \\in [0,1]$.\n",
    "\n",
    "Note that most of the formulas from the slides for the Mixture of Gaussians problem can be reused, as they are not specific to the Gaussian distribution, but for a Mixture Model in general. These calculations shouldn't be reproduced again, just use the corresponding results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation (35 points):</h3>\n",
    "\n",
    "Your calculation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\"> Task 3: Implementation of Mixture of Poisson algorithm </h2>\n",
    "\n",
    "\n",
    "Now we intend to write a program that estimates all the parameters of a Mixture of Poissons via an EM algorithm by using the results from the previous task. If you have done that correctly, you should have obtained $\\alpha_k = \\frac{1}{n}\\sum_{i=1}^n p(u_i\\! =\\! k \\!\\mid\\! x_i)$, $ \\lambda_k = \\frac{\\sum_{i=1}^nx_i p(u_i = k \\mid x_i)}{\\sum_{i=1}^np(u_i = k \\mid x_i)}$, and $\n",
    "r_{ik} =\\frac{\\alpha_k p(x_i;\\theta_k)}{\\sum_{l=1}^K\\alpha_l p(x_i;\\theta_l)}\n",
    "$. (Obviously, it's not enough for the previous task to just write down these solutions, of course you have to provide arguments.) Let us start with reading in and visualizing the data set that you have to use, then proceed with doing the following tasks:\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Task:</h3>\n",
    "\n",
    "* Write functions that implement the E-step, M-step and the log-likelihoods.\n",
    "* Then iterate until the differences between the log-likelihoods before and after some iteration step is less than $10^{-5}$.\n",
    "* Let your algorithm run on the data from $\\mathsf{cnvdata.csv}$, which we already encoded in the variable data. Start with $K=4$ mixture components, and initialize all four alphas equally. For the initialization of the $\\lambda_k$'s use $10,50,100$ and $150$. \n",
    "* Try further choices of $K$ and run different experiments with different random initializations (at least one). Be careful with exponents and factorials to not run into overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nothing to do here\n",
    "\n",
    "from scipy.stats import poisson\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt(open(\"cnvdata.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "plt.hist(data,bins=50)\n",
    "plt.title('Data distribution')\n",
    "plt.ylabel('$p(X)$')\n",
    "plt.xlabel('$X$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code (30 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Begin your code (multiple cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End your code (multiple cells)\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Task:</h3>\n",
    "\n",
    "* Test your implementation of the four components with the mentioned initializations. Print the corresponding $\\alpha$'s, $\\lambda$'s and log-likelihoods. Finally, produce a plot that visualizes the original data distribution and the estimated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code (10 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Begin your code (multiple cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End your code (multiple cells)\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Task:</h3>\n",
    "\n",
    "* How many different copy numbers (i.e. components or clusters) do you think are in the data? What do you observe, and how do different values of $K$ change the results of your algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question (5 points):</h3>\n",
    "\n",
    "Your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
